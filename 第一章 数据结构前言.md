[toc]

## 算法与数据结构

&emsp;**数据结构**研究信息的存储和处理方法。随着计算学科的发展，计算机的应用已经深入到各个领域。计算机处理的数据已不再局限于整型、实型等数值型数据，还可以是字符、表格、声音、图像等非数值型数据。数值计算的特点是数据类型简单、算法复杂，所以更侧重于程序设计的技巧。而非数值计算的特点是数据之间的关系复杂，数据量十分庞大，要设计出好的非数值计算的程序必须解决下列问题：
* 明确所处理的数据之间的逻辑关系以及处理要求。数据之间的逻辑关系包括两个层次：每个数据元素的组成，以及数据元素之间的关系。
* 如何将数据存储在计算机中。保存数据也要保存两个方面的内容：数据元素的保存以及数据元索之间的关系的保存。
* 如何实现数据的处理。数据元素之间的关系有各种保存方法，对每一种保存方法，数据处理的过程是不同的。每个数据处理的过程就是一个算法。

&emsp;数据结构的讨论可以分为两个层次：抽象层和实现层。抽象层讨论数据的逻辑结构和所需的运算；实现层讨论数据的存储表示及运算的实现。

### 数据的逻辑结构

&emsp;从概念上讲，一个数据结构是由一组同类的数据元素依据某种联系组织起来的。数据元素间的逻辑关系的描述称为数据的逻辑结构。不管应用如何变化，从抽象层面上看，数据的逻辑结构有下列4种：

* 集合结构：元素间的次序是任意的。元素之间除了“属于同一集合”的联系外没有其他的关系。
* 线性结构：数据元素之间构成一个有序序列。其中第一个元素只有后继没有前驱，最后一个元素只有前驱没有后继。除了第一个和最后一个元素外，其余元素都有一个前驱和一个后继。
* 树形结构：除了一个特殊的根元素外，每个元素有且仅有一个前驱，后继数目不限，根元素没有前驱。树形结构表示的是一种层次关系。
* 图形结构：图是最一般的逻辑结构，图中的每个元素的前驱和后继数目都不限。

> 有时也把线性结构以外的其他3种结构称为非线性结构。

### 数据结构的运算

&emsp;每种逻辑结构都有一定的处理要求，这些处理要求被称为数据结构的操作或运算。数据结构最常见的运算有如下几种：

* 创建运算（create）：创建一个空的数据结构。
* 清除运算（clear）：删除数据结构中的所有数据元素。
* 插入运算（insert）：在数据结构指定的位置上插入一个新数据元素。
* 删除运算（remove）：将数据结构中的某个数据元素删去。
* 搜索运算（search）：在数据结构中搜索满足特定条件的数据元素。
* 更新运算（update）：修改数据结构中的某个数据元素的值。
* 访问运算（visit）：访问数据结构中的某个数据元素。
* 遍历运算（traverse）：按照某种次序访问数据结构中的每一个数据元素，使每个数据元素恰好被访问一次。

> 除了上述运算之外，每种数据结构还可以包含一些特定的运算。

### 存储实现
&emsp;存储实现的基本目标是建立数据的机内表示，包括两个部分：数据元素的存储和数据元素之间的关系的存储。有时为了方便运算的实现，还可能会增加一些辅助信息的存储。数据的机内表示称为数据的物理结构，由以下3个部分组成：

* 存储结点，每个存储结点存放一个数据元素。
* 数据元素之间的关系的存储，也就是逻辑结构的机内表示。
* 附加信息，为方便运算的实现而设置的一些哑结点，如链表中的头结点。

&emsp;物理结构主要讨论的是数据元素之间的关系的表示。由于每个数据元素被表示为一个存储结点，所以逻辑结构就由存储结点之间的关联方式间接地表示。通常存储结点之间的关联有以下4种实现方式：

* 顺序实现：所有的存储结点存放在一块连续的存储区域中，结点之间的逻辑关系可以通过结点的存储位置来体现。在高级语言中一块连续的存储空间通常可以用一个数组来表示。因此顺序实现通常是用一个数据元素类型的数组来存储数据。
* 链接实现：存储结点可以离散地存放在存储器的不同位置，结点之间的关系通过一个指针显式地指出。因此，在链接实现中每个存储节点包含数据元素部分和指针部分。
* 散列存储方式：是专用于集合结构的数据存储方式。在散列存储中，各个结点均匀地分布在一块连续的存储区域中，用一个散列函数将数据元素和存储位置关联起来。
* 索引存储方式：所有的存储结点按照生成的次序连续存放。另外设置一个索引区域表示结点之间的关系。


## 算法分析

&emsp;一般而言，对同一个问题可以设计出不同的解决方法。因此对于一个运算也可以有不同的实现，即可以设计出实现该运算的不同算法。这样就产生了如何评价这些算法的问题。通过这种评价，设计人员可以区分不同实现的相对的优劣，从而选择一个较好的算法。通常可以从以下几个方面来评价算法的质量：

* 正确性：算法应能正确地实现预定的功能。
* 易读性：算法应易于阅读和理解，以便于调试、修改和扩充。
* 健壮性：当环境发生变化（如遇到非法输入）时，算法能适当地做出反应或进行处理，不会产生不正确的运算结果。
* 高效率：具有较高的时间和空间性能。

&emsp;这些指标往往是互相冲突的，数据结构着重讨论算法的时空性能。确定算法的时空性能通常被称为算法分析。算法的时空性能是指算法的时间性能（或称时间复杂度）和空间性能（或称空间复杂度）。前者指算法包含的计算量，后者指算法需要的存储量。

### 时间复杂度的概念

&emsp;算法的具体实现是程序。程序的运行时间是程序运行从开始到结束所需要的时间。影响程序运行时间的因素很多，主要有以下几个：

* 问题规模和输入数据的分布。
* 编译器生成的目标代码的质量。
* 计算机系统的性能。
* 程序采用的算法的优劣。

&emsp;当在不同的计算机上运行同一个程序，输入同样的数据，所需的时间是不同的。程序运行所需的时间取决于计算机软、硬件系统。
&emsp;衡量一个算法的好坏不能看在某一台计算机上运行时间的长短，而要抽取算法的本质，即运算量与问题规模之间的关系。算法所需的运算量与问题规模之间的关系被称为**算法的时间复杂度**。算法的时间复杂度是一种抽象的度量，优化算法的时间复杂度才是降低运行时间的关键。在算法分析中，通常用最好情况的时间复杂度（best case time complexity）、最坏情况的时间复杂度（worst case time complexity）以及平均情况的时间复杂度（average case time complexity）来描述。最好情况下的时间性能不足以说明问题。平均的时间性能比较难算，有时甚至定义平均本身就已经很难，因此在时间性能衡量时，通常采用比较容易分析和计算并且也最具有实际价值的**最坏情况下的时间性能**。

### 渐进表示法

&emsp;在算法分析中，不考虑具体的运行时间函数，只考虑运行时间函数的数量级，这种方法称为**渐进表示法**。最常用的渐进表示法是大$O$表示法。
&emsp;如果存在两个正常数$c$和$N_o$，使得当$N \ge N_o$时有$T(N) \le cF(N)$，则记为$T(N) = O(F(N))$。大$O$表示法给出了算法在问题规模$n$达到一定程度后运行时间增长率的上界，因此被称为渐进时间复杂度，简称为时间复杂度。大$O$表示法的$O$是单词Order​的首字母，表示数量级。因此大$O$表示法并不需要给出运行时间的精确值而只需要给出一个数量级，表示当问题规模很大时算法运行时间的增长是受限于哪一个数量级的函数，所以在选择$F(N)$时通常选择的是比较简单的函数形式，并忽略低次项和系数。常用的时间复杂度函数及名称如下：

| 函数  | 名称  |
| ----- | ----- |
|   1   | 常量  |
| $\log N$  | 对数  |
| $N$  | 线性  |
| $N\log N$ | $N\log N$ |
| $N^2$  | 平方  |
| $N^3$  | 立方  |
| $2^N$  | 指数  |
| $N!$  | 指数  |
| $N^N$ | 指数  |

&emsp;时间复杂度为多项式的算法称为**多项式时间算法**，时间复杂度为指数函数的算法称为**指数时间算法**。最常见的多项式时间算法的时间复杂度关系为：
$$
O(1) < O(\log N) < O(N) < O(N\log N) < O(N^2) < O(N^3)
$$
指数时间算法的时间复杂度关系为：
$$
O(2^N) < O(N!) < O(N^N)
$$
而多项式时间的算法要比指数时间的算法好。
&emsp;大$O$表示法用以表达算法运行时间的上限。当说一个算法具有$O(F(n))$的运行时间时，是指当$n$足够大时，该算法在计算机上的实际运行时间不会超过$F(n)$的常数倍，即是算法运行时间的上界。但大$O$表示法并没有给出这个函数和这个上界的接近程度。这意味着这个上界既可能非常接近，也可能相距很远。在实际应用中，通常取运行时间函数的主项。为了更精确地表示算法的时间性能，还定义了三种时间复杂度的表示法：
* 大$\Omega$：如果存在正常数$c$和$N_o$，满足当$N \ge N_o$时有$T(N) \ge cF(N)$，则$T(N)$是$\Omega(F(N))$。
* 大$\Theta$：当且仅当$T(N)$是$O(F(N))$，并且$T(N)$又是$\Omega(F(N))$，则$T(N)$是$\Theta(F(N))$。
* 小$o$：当且仅当$T(N)$是$O(F(N))$，并且$T(N)$不是$\Theta(F(N))$，则$T(N)$是$o(F(N))$。

如果用传统的关系运算符来比较增长率，那么大$O$表示法说明$T(N)$的增长率小于或等于$F(N)$的增长率；大$\Omega$表示法说明$T(N)$的增长率大于或等于$F(N)$的增长率。大$\Theta$表示法说明$T(N)$的增长率等于$F(N)$的增长率。小$o$表示法说明$T(N)$的增长率严格小于$F(N)$的增长率。

### 时间复杂度的计算

* 求和定理：

假定$T_1(n)$和$T_2(n)$是程序段$P_1$、$P_2$的运行时间，并且$T_1(n)$是$O(f(n))$，而$T_2(n)$是$O(g(n))$。那么先运行$P_1$再运行$P_2$的总的运行时间是$T_1(n) + T_2(n) = O(\max\{f(n),g(n)\})$。

> 此处假设数据集都是$n$，如果运行时间为$T_1(m)$、$T_2(n)$，则不能简单的使用加法定理，因为没法评估$T_1(m)$和$T_2(n)$的数量级，这时使用$T_1(m) + T_2(n) = O(f(m) + g(n))$。

* 求积定理：

如果$T_1(n)$和$T_2(n)$分别是$O(f(n))$和$O(g(n))$，那么$T_1(n) \times T_2(n)$是$O(f(n) \times g(n))$。

&emsp;由以上两个定理可以得到如下计算规则：

* 规则1：简单语句，如赋值语句、输入输出语旬，它们的运行时间与问题规模无关，在每个计算 机系统中运行时间都是一个常 量，因此时间复杂度为$O(1)$。

* 规则2：条件语句，`if <条件> then <语句> else <语>`的运行时间为执行条件判断的代价，一般为$O(1)$再加上执行`then`后语句的代价（若条件为真），或执行`else`后面的语句的代价（若条件为假）之和，即$\max\{O(then子句),O(else子句)\}$。

* 规则3：循环语句往往与问题规模有关，是分析的重点。循环语句的执行时间是循环控制行和循环体执行时间的总和。循环控制一般是一个简单的条件判断，因此循环语句的执行时间是循环体的运行时间乘循环次数。

* 规则4：嵌套循环语旬，对外层循环的每个循环周期，内存循环都要执行它的所有循环周期，因此可用求积定理计算整个循环的时间复杂度，即最内层循环体的运行时间乘以 所有循环的循环次数。

* 规则5：连续语句，利用求和定理把这些语句的时间复杂度相加。